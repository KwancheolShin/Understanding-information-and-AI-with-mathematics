{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY4v1oKqOgp0VsxDYJwexp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KwancheolShin/Understanding-information-and-AI-with-mathematics/blob/main/XOR_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW7Bc7ZtZlGe"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Jun  4 12:13:16 2025\n",
        "\n",
        "@author: kcshi\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "x_train = np.array([[1,1],[0,0],[0,1],[1,0]])\n",
        "t_train = np.array([[1,0],[1,0],[0,1],[0,1]])\n",
        "\n",
        "\n",
        "# define classes for layers\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "        self.out  = None\n",
        "\n",
        "    def forward(self,x):\n",
        "        # self.mask = (x<=0)\n",
        "        # out = x.copy()\n",
        "        # out[self.mask]=0\n",
        "        self.out = np.where(x>0,x,0)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self,dout):\n",
        "        # dout[self.mask] =0\n",
        "        # dx = dout\n",
        "        dx = np.where(self.out>0,dout,0)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = 1/(1+np.exp(-x))\n",
        "        self.out = out\n",
        "\n",
        "        return self.out\n",
        "\n",
        "    def backward(self,dout):\n",
        "        dx = self.out*(1.0-self.out)*dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self,W,b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self,x):\n",
        "        self.x = x\n",
        "        out  = np.matmul(self.x,self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self,dLdy):\n",
        "        h = dLdy.shape[0]\n",
        "        dLdx = np.matmul(dLdy,self.W.T)\n",
        "        self.dW = np.matmul(self.x.T,dLdy)/h\n",
        "        self.db = np.sum(dLdy,axis=0)/h\n",
        "\n",
        "        return dLdx\n",
        "\n",
        "def softmax(x):\n",
        "    x = x-np.max(x,axis=1,keepdims=True)\n",
        "    exp_x = np.exp(x)\n",
        "\n",
        "    return exp_x/np.sum(exp_x,axis=1,keepdims=True)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y,t):\n",
        "    #y: output of the Neural network\n",
        "    #t: label\n",
        "    h = y.shape[0] # mini-batch size\n",
        "\n",
        "    return -np.sum(t*np.log(y + 1e-7))/h\n",
        "\n",
        "\n",
        "class SoftmaxWithCrossEntropy: # Cross Entropy loss\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self,x,t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y,self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self,dout=1):\n",
        "        #batch_size = self.t.shape[0]\n",
        "        dx = (self.y - self.t)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithMSE: # Mean-Square-Error loss\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None # Softmax 결과\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self,x,t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = 0.5*np.mean((self.y - self.t)**2)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self,dout=1):\n",
        "        B, K = self.t.shape\n",
        "        dx = np.zeros_like(self.y)\n",
        "\n",
        "        for i in range(B): # 미니배치에 대한 연산을 한꺼번에 하기는 힘들고 for loop를 돌려야하는 것 같음 =>Cross-Entropy 보다 계산이 많음\n",
        "            y_i  = self.y[i] # shape: (K,)\n",
        "            t_i  = self.t[i] # shape: (K,)\n",
        "            dy_i = y_i - t_i # shape: (K,)\n",
        "\n",
        "            J = np.diag(y_i) - np.outer(y_i,y_i)\n",
        "            dx[i] = np.matmul(J, dy_i) #shape: (K,)\n",
        "\n",
        "        dx = (dout/B)*dx\n",
        "        return dx\n",
        "\n",
        "#define Neural Network Architecture\n",
        "class TwoLayerNet:\n",
        "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
        "        #__init__ 함수를 이용해서 클래스 속성변수를 초기화하는 부분\n",
        "        self.params ={} #empty dictionary to save trainable parameters below\n",
        "        # self.params['W1'] = weight_init_std*np.random.randn(input_size,hidden_size) # random initialization\n",
        "        # self.params['b1'] = np.zeros(hidden_size) # zero initialization\n",
        "        # self.params['W2'] = weight_init_std*np.random.randn(hidden_size,output_size)\n",
        "        # self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        self.params['W1'] = np.array([[2,-1],[-2,2]], dtype=float) # random initialization\n",
        "        self.params['b1'] = np.array([1,-1], dtype=float) # zero initialization\n",
        "        self.params['W2'] = np.array([[-1,1],[-2,2]], dtype=float)\n",
        "        self.params['b2'] = np.array([1,-1], dtype=float)\n",
        "\n",
        "        self.grads = {}  #empty dictionary to save gradients of the parameters\n",
        "\n",
        "        #define layers\n",
        "        self.Affine1   = Affine(self.params['W1'],self.params['b1']) # 1_st hidden layer\n",
        "        self.actv      = Relu()                                      # activation/ Choose either Relu or Sigmoid class\n",
        "        self.Affine2   = Affine(self.params['W2'],self.params['b2']) # 2_nd hidden layer\n",
        "        self.lastLayer = SoftmaxWithCrossEntropy()                  # last layer: softmax layers + Cross-Entropy loss layer\n",
        "        #self.lastLayer = SoftmaxWithMSE()                          # last layer: softmax layers + MSE loss layer\n",
        "\n",
        "    #순전파 함수\n",
        "    def forward(self,x,t):\n",
        "        a1 = self.Affine1.forward(x)\n",
        "        z1 = self.actv.forward(a1)\n",
        "        a2 = self.Affine2.forward(z1)\n",
        "        loss  = self.lastLayer.forward(a2,t)\n",
        "        return a2, loss\n",
        "\n",
        "    #역전파 함수\n",
        "    def backward(self,x,t):\n",
        "        dout = 1\n",
        "        douta2 = self.lastLayer.backward(dout)\n",
        "        doutz1 = self.Affine2.backward(douta2)\n",
        "        douta1 = self.actv.backward(doutz1)\n",
        "        doutx  = self.Affine1.backward(douta1)\n",
        "\n",
        "        self.grads['dW1'] = self.Affine1.dW\n",
        "        self.grads['db1'] = self.Affine1.db\n",
        "        self.grads['dW2'] = self.Affine2.dW\n",
        "        self.grads['db2'] = self.Affine2.db\n",
        "\n",
        "        return self.grads\n",
        "\n",
        "    #경사하강법 함수\n",
        "    def step(self,lr): # update parameters by gradient decent step\n",
        "        self.Affine1.W -= lr*self.Affine1.dW\n",
        "        self.Affine1.b -= lr*self.Affine1.db\n",
        "        self.Affine2.W -= lr*self.Affine2.dW\n",
        "        self.Affine2.b -= lr*self.Affine2.db\n",
        "\n",
        "        self.params['W1'] = self.Affine1.W\n",
        "        self.params['b1'] = self.Affine1.b\n",
        "        self.params['W2'] = self.Affine2.W\n",
        "        self.params['b2'] = self.Affine2.b\n",
        "\n",
        "    #훈련과정에서 정확도를 측정하는 함수\n",
        "    def accuracy(self,x,t):\n",
        "        y,loss = self.forward(x,t)\n",
        "        y = np.argmax(y,axis=1)\n",
        "        if t.ndim !=1 : t= np.argmax(t,axis = 1)\n",
        "\n",
        "        accuracy= np.sum(y==t)/float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    #훈련이 끝난 뒤, 훈련된 네트워크를 이용하여 숫자를 판별하는 함수\n",
        "    def predict(self,sample_id):\n",
        "        x = x_train[sample_id].reshape(1, -1)\n",
        "        t = t_train[sample_id].reshape(1, -1)\n",
        "        y, _ = self.forward(x, t)\n",
        "\n",
        "        t_predict = np.argmax(y)\n",
        "        t_true = np.argmax(t)\n",
        "\n",
        "        print(\"predicted value is: \", t_predict)\n",
        "        print(\"true target is: \", t_true)\n",
        "\n",
        "# 이제 class로 구조를 다 만들었으므로 본격적으로 네트워크를 하나 만들어서 훈련을 시작함\n",
        "\n",
        "# define a network instance: 훈련할 네트워크를 하나 만듦\n",
        "network = TwoLayerNet(input_size = 2, hidden_size=2, output_size=2)\n",
        "\n",
        "# training hyper-parameters: 하이퍼파라미터 설정\n",
        "iters_num = 1\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 1\n",
        "learning_rate = 1\n",
        "\n",
        "#훈련과정을 모니터링하기 위해 빈 리스트를 만들어 훈련과정을 기록함\n",
        "train_loss_list = []\n",
        "train_acc_list  = []\n",
        "\n",
        "iter_per_epoch = 10\n",
        "\n",
        "\n",
        "print(\"Training started ...\")\n",
        "#미니배치 학습 시작\n",
        "for i in range(iters_num):\n",
        "    for j in range(4):\n",
        "        x_batch = x_train[j].reshape(1,-1) # 슬라이싱을 이용하여 미니배치 크기만큼 뽑아냄\n",
        "        t_batch = t_train[j].reshape(1,-1)\n",
        "\n",
        "        _, loss  = network.forward(x_batch,t_batch)  # 순전파 시행\n",
        "        grads    = network.backward(x_batch,t_batch) # 역전파 시행\n",
        "        network.step(learning_rate)                               # 경사하강법 시행\n",
        "\n",
        "        train_loss_list.append(loss)                 # 손실이 잘 줄어드는지 보기위해 매 훈련간 loss를 저장함\n",
        "\n",
        "        print(network.params)\n",
        "        if i%iter_per_epoch == 0: # 매 iter_per_epoch 마다,\n",
        "            train_acc = network.accuracy(x_train,t_train) # 훈련 데이터를 얼마나 정확하기 학습하는지 iter_per_epoch 마다 정확도를 측정\n",
        "            train_acc_list.append(train_acc)              # 누가 기록\n",
        "            print(f\"epoch:{i:6d}, Train Acc: {train_acc:.5f}, Loss: {loss:.5f}\") # 훈련 상황을 출력\n",
        "\n",
        "\n",
        "print(\"Training ended \\n\")\n",
        "print(\"==============\\n\\n\")\n",
        "\n",
        "# print(\"size of W1 =\", network.Affine1.W.size)\n",
        "# print(\"size of b1 =\", network.Affine1.b.size)\n",
        "# print(\"size of W2 =\", network.Affine2.W.size)\n",
        "# print(\"size of b2 =\", network.Affine2.b.size)\n",
        "\n",
        "\n",
        "# print(\"Network parameters: \\n\",network.params)\n",
        "# print(\"\\n\\n ====================================================== \\n\\n\")\n",
        "# print(\"Network parameter's gradients: \\n\",network.grads)\n",
        "\n",
        "\n",
        "# 손실 그래프\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_list, label='Train Loss')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# 정확도 그래프 (Train vs Test)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_acc_list, label='Train Accuracy')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train/Test Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# test a sample\n",
        "sample_id = 1 #pick a number\n",
        "network.predict(sample_id)"
      ]
    }
  ]
}